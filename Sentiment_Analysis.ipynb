{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMkzWAO0LUYTjptMaYOsGbU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronjoseph/AB_Testing/blob/master/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9SY-zfsEwlH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2918f69d-a9e7-4e55-a388-8fb2b146ad6f"
      },
      "source": [
        "#Load the libraries\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import nltk\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.preprocessing import LabelBinarizer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "from wordcloud import WordCloud,STOPWORDS\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import spacy\r\n",
        "import re,string,unicodedata\r\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\r\n",
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer\r\n",
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.svm import SVC\r\n",
        "from textblob import TextBlob\r\n",
        "from textblob import Word\r\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\r\n",
        "import os\r\n",
        "from tqdm import tqdm\r\n",
        "tqdm.pandas()\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "nltk.download('stopwords')\r\n",
        "  "
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXNLLRpmFwQc",
        "outputId": "6c0d77bb-6524-429c-ac34-b1d97af16597"
      },
      "source": [
        "imdb_data = pd.read_csv('IMDB Dataset.csv',error_bad_lines=False,engine='python')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping line 31643: unexpected end of data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl-26IlME3lF"
      },
      "source": [
        "# Data Cleaning Process\r\n",
        "def preprocess(text):\r\n",
        "  text=text.lower()\r\n",
        "  # remove hyperlinks\r\n",
        "  text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\r\n",
        "  text = re.sub(r'http?:\\/\\/.*[\\r\\n]*', '', text)\r\n",
        "  #remove hashtag sign\r\n",
        "  text=re.sub(r\"#\",\"\",text)   \r\n",
        "  #remove mentions\r\n",
        "  text = re.sub(r\"(?:\\@)\\w+\", '', text)\r\n",
        "  #remove non ascii chars\r\n",
        "  text=text.encode(\"ascii\",errors=\"ignore\").decode()\r\n",
        "  #remove some puncts (except . ! ?)\r\n",
        "  text=re.sub(r'[:\"#$%&\\*+,-/:;<=>@\\\\^_`{|}~]+','',text)\r\n",
        "  text=re.sub(r'[!]+','!',text)\r\n",
        "  text=re.sub(r'[?]+','?',text)\r\n",
        "  text=re.sub(r'[.]+','.',text)\r\n",
        "  text=re.sub(r\"'\",\"\",text)\r\n",
        "  text=re.sub(r\"\\(\",\"\",text)\r\n",
        "  text=re.sub(r\"\\)\",\"\",text)    \r\n",
        "  text=\" \".join(text.split())\r\n",
        "  return text\r\n",
        "\r\n",
        "def simple_stemmer(text):\r\n",
        "  ps=nltk.porter.PorterStemmer()\r\n",
        "  text= ' '.join([ps.stem(word) for word in text.split()])\r\n",
        "  return text\r\n",
        "\r\n",
        "#removing the stopwords\r\n",
        "def remove_stopwords(text, is_lower_case=False):\r\n",
        "  #set stopwords to english\r\n",
        "  stop=set(stopwords.words('english'))\r\n",
        "  # Tokenization\r\n",
        "  tokenizer=ToktokTokenizer()\r\n",
        "  #Setting English stopwords\r\n",
        "  stopword_list=nltk.corpus.stopwords.words('english')\r\n",
        "  tokens = tokenizer.tokenize(text)\r\n",
        "  tokens = [token.strip() for token in tokens]\r\n",
        "  if is_lower_case:\r\n",
        "      filtered_tokens = [token for token in tokens if token not in stopword_list]\r\n",
        "  else:\r\n",
        "      filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\r\n",
        "  filtered_text = ' '.join(filtered_tokens)    \r\n",
        "  return filtered_text"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfrb3kJRKxbM",
        "outputId": "2227747d-b3fa-4a62-bc32-72d738f7793a"
      },
      "source": [
        "#Apply function\r\n",
        "imdb_data['clean']=imdb_data['review'].progress_apply(preprocess)\r\n",
        "imdb_data['stem']=imdb_data['clean'].progress_apply(simple_stemmer)\r\n",
        "imdb_data['token'] = imdb_data['stem'].progress_apply(remove_stopwords) "
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31641/31641 [00:04<00:00, 6874.62it/s]\n",
            "100%|██████████| 31641/31641 [01:59<00:00, 265.41it/s]\n",
            "100%|██████████| 31641/31641 [00:36<00:00, 859.50it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKEeR8ehNqED"
      },
      "source": [
        "# Bag of Words & TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGj5J_dvNvWX"
      },
      "source": [
        "# Splitting the data\r\n",
        "X = imdb_data['token']\r\n",
        "y = imdb_data['sentiment']\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42) "
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "derKOiekNvcA",
        "outputId": "bbabb982-8e55-471e-d0f3-6bf1f8c6cea8"
      },
      "source": [
        ""
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21941    thi one best film ive seen last yearsbelmonndo...\n",
              "3207     star rate saturday night friday night friday m...\n",
              "2319     film manag surviv almost origin alon wonderlan...\n",
              "17226    took three attempt get thi movi although total...\n",
              "31498    dure sleepless night wa switch channel found t...\n",
              "                               ...                        \n",
              "29802    wa first introduc thi movi san antonio tx thi ...\n",
              "5390     cant knock thi film terribl becaus obviou midw...\n",
              "860      thi product wa quit surpris absolut love obscu...\n",
              "15795    thi decent movi although littl bit short time ...\n",
              "23654    anoth veri good mann flick thank fatherson com...\n",
              "Name: token, Length: 25312, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S6etRQzNvgZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMnl3dpuNvkm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Towmxa0eNvo9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}